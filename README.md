# Explore Importance of Features in Random Forests

Random forests are among the most popular machine learning methods thanks to their relatively good accuracy, robustness and ease of use. They also provide two straightforward methods for feature selection:

- Gini Importance or Mean Decrease in Impurity (MDI)
- Permutation Importance or Mean Decrease in Accuracy (MDA)

A novel all-relevant feature selection method is:
- Boruta, conceived by Witold R. Rudnicki and developed by Miron B. Kursa at the Interdisciplinary Centre for Mathematical and Computational Modelling at the University of Warsaw (ICM UW)
